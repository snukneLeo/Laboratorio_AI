{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-LAB SESSION 4: Tutorial\n",
    "\n",
    "In this tutorial we will see some additional functionalities available to OpenAI Gym environments\n",
    "\n",
    "## Cliff environment\n",
    "\n",
    "The environment used is **Cliff** (taken from the book of Sutton and Barto as visible in the figure)\n",
    "![CliffWalking](images/cliff.png)\n",
    "\n",
    "The agent starts in cell $(3, 0)$ and has to reach the goal in $(3, 11)$. Falling from the cliff resets the position to the start state (the episode ends only when the goal state is reached). All other cells are safe. Action dinamycs is deterministic, meaning that the agent always reaches the desired next state (although the agent does not have access to this information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import gym\n",
    "import envs\n",
    "import numpy as np\n",
    "from utils.funcs import run_episode, plot\n",
    "\n",
    "env = gym.make(\"Cliff-v0\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell types are the following:\n",
    "* *x* - Start position\n",
    "* *o* - Safe\n",
    "* *C* - Cliff\n",
    "* *T* - Goal\n",
    "\n",
    "Rewards:\n",
    "- <span style=\"color:orange\">-1</span> for each \"safe\" cell (o)\n",
    "- <span style=\"color:red\">-100</span> for falling from the cliff (C)\n",
    "\n",
    "In addition to the functionalities of the environments you have been using in the previous sessions, there are also a few more:\n",
    "- *step(action)*: the agent performs *action* from the current state. Returns a tuple *(new_state, reward, done, info)* where:\n",
    "    - *new_state*: is the new state reached as a consequence of the agent's last action\n",
    "    - *reward*: the reward obtained by the agent\n",
    "    - *done*: `True` if the episode has ended, `False` otherwise\n",
    "    - *info*: not used, you can safely discard it\n",
    "- *reset()*: the environment is reset and the agent goes back to the starting position. Returns the initial state id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, -1, False, {'prob': 1.0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "env.step(0)  # Go UP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "The environment is not known a-priori, hence it does not have the following properties and methods available:\n",
    "* *T(s, a, s')*: no transition matrix\n",
    "* *R(s, a, s')*: no reward matrix\n",
    "\n",
    "The action ids are different from the previous environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'U', 1: 'R', 2: 'D', 3: 'L'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to execute a random policy in the environment: we create such policy as usual, we reset the environment to its initial state and also set a maximum number of steps for the episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.random.choice(env.action_space.n, env.observation_space.n)\n",
    "state = env.reset()\n",
    "ep_limit = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we execute a loop where at each iteration a step is performed by using the action defined by the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-515"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "el = 0\n",
    "total_reward = 0\n",
    "\n",
    "# Episode execution loop\n",
    "for _ in range(ep_limit):\n",
    "    next_state, reward, done, _ = env.step(policy[state])  # Execute a step\n",
    "    total_reward += reward\n",
    "    el += 1\n",
    "    if done or el == ep_limit:  # If done == True, the episode has ended\n",
    "        break\n",
    "    state = next_state\n",
    "    \n",
    "total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful operation that can be done with numpy is dividing 2 $n$-dimensional arrays element wise for all the positions where the denominator is $\\neq 0$. For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array A:\n",
      " [[10. 10.  8.]\n",
      " [10. 10. 20.]]\n",
      "\n",
      "Array B:\n",
      " [[2. 2. 0.]\n",
      " [2. 2. 0.]]\n",
      "\n",
      "Division result:\n",
      " [[ 5.  5.  8.]\n",
      " [ 5.  5. 20.]]\n"
     ]
    }
   ],
   "source": [
    "a = np.asarray([[10, 10, 8], [10, 10, 20]], dtype=\"float16\")\n",
    "b = np.asarray([[2, 2, 0], [2, 2, 0]], dtype=\"float16\")\n",
    "print(\"Array A:\\n\", a)\n",
    "print(\"\\nArray B:\\n\", b)\n",
    "np.divide(a, b, out=a, where=b != 0)\n",
    "print(\"\\nDivision result:\\n\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the result of the operation is stored in A (its original value is lost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
